# Copyright (c) Facebook, Inc. and its affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: Linux Benchmark

on:
  pull_request:
  # only running on changes to source files would be possible
  push:
    branches: [main]

permissions:
  contents: read

defaults:
  run:
    shell: bash

jobs:
  benchmark:
    runs-on: ubuntu-22.04
    container: ghcr.io/facebookincubator/velox-dev:amd64-ubuntu-22.04-avx
    env:
      CCACHE_DIR: "${{ github.workspace }}/.ccache/"
      BINARY_DIR: "${{ github.workspace }}/benchmarks/"
      CONBENCH_URL: "https://velox-conbench.voltrondata.run/"
      CONBENCH_HOST: "GitHub-runner"
      LINUX_DISTRO: "ubuntu"
      RESULTS_ROOT: "${{ github.workspace }}/benchmark-results"
      BASELINE_OUTPUT_PATH: "${{ github.workspace }}/benchmark-results/baseline/"
      TARGET_OUTPUT_PATH: "${{ github.workspace }}/benchmark-results/target/"
    steps:
      - run: |
            # Set up xml gtest output.
            mkdir -p /tmp/test_xml_output/
            echo "XML_OUTPUT_FILE=/tmp/test_xml_output/" >> $GITHUB_ENV

            # Set up ccache configs .
            mkdir -p .ccache
            ccache -sz -M 5Gi

      - name: "Checkout Base"
        uses: actions/checkout@v3
        with:
          repository: "facebookincubator/velox"
          ref: "e1fe3ce469d47a2ad1355afa952646b1f098e3a3"
          path: 'base'
          submodules: 'recursive'

      - name: "Checkout Head"
        uses: actions/checkout@v3
        with:
          path: 'head'
          submodules: 'recursive'
      
      - run: |
          apt update
          apt install -y lsb-release python3 pip

      # - uses: actions/setup-python@v4
      #   with:
      #     python-version: '3.8'
      #     cache: 'pip'
      #     cache-dependency-path: 'head/scripts/*'
      
      # - name: "Setup Ubuntu"
      #   run: |
      #     # install libunwind explicitly to avoid dependecy hell
      #     # https://bugs.launchpad.net/ubuntu/+source/google-glog/+bug/1991919
      #     sudo apt install -y libunwind-dev

      #     # Run the setup script from HEAD so changes from PRs are reflected.
      #     sudo ./head/scripts/setup-ubuntu.sh

      - name: build-benchmarks
        run: |
          build_benchmarks() {
            echo "::group::Build $2 Benchmarks"
            pushd $1
            n_cores=$(nproc)
            make benchmarks-basic-build NUM_THREADS=$n_cores MAX_HIGH_MEM_JOBS=$((n_cores/2)) MAX_LINK_JOBS=$n_cores
            ccache -s
            mkdir -p $3
            cp -r --verbose _build/release/velox/benchmarks/basic/* $3
            echo "::endgroup::"
            popd
          }

          build_benchmarks base Baseline ${BINARY_DIR}/baseline/
          build_benchmarks head Target ${BINARY_DIR}/target/

      - name: "Install benchmark dependencies"
        run: |
          cd head
          python3 -m pip install -r scripts/benchmark-requirements.txt

      - name: "Run Benchmarks - Full Round"
        working-directory: 'head'
        run: |
            # TODO: For now we just compare the target binary with itself, until we
            # stabilize benchmark runs. We should eventually have two different
            # set of binaries.
            make benchmarks-basic-run \
                EXTRA_BENCHMARK_FLAGS="--binary_path ${BINARY_DIR}/baseline/ --output_path ${BASELINE_OUTPUT_PATH}"
            make benchmarks-basic-run \
                EXTRA_BENCHMARK_FLAGS="--binary_path ${BINARY_DIR}/target/ --output_path ${TARGET_OUTPUT_PATH}"

            ./scripts/benchmark-runner.py compare \
                --baseline_path ${BASELINE_OUTPUT_PATH} \
                --target_path ${TARGET_OUTPUT_PATH} \
                --rerun_json_output "${RESULTS_ROOT}/rerun_json_output_0.json" \
                --do_not_fail
      - name: "Run Benchmarks - Reruns"
        working-directory: 'head'
        run: |
            for i in 1 2 3 4 5; do
              CURRENT_JSON_RERUN="${RESULTS_ROOT}/rerun_json_output_$((${i} - 1)).json"
              NEXT_JSON_RERUN="${RESULTS_ROOT}/rerun_json_output_${i}.json"
              if [ ! -s "${CURRENT_JSON_RERUN}" ]; then
                echo "--> Rerun iteration ${i} found empty file. Finalizing."
                break
              fi
              echo "--> Starting rerun iteration: ${i}"
              make benchmarks-basic-run \
                  EXTRA_BENCHMARK_FLAGS="--binary_path _build/benchmarks/baseline/ --output_path ${BASELINE_OUTPUT_PATH}/rerun-${i}/ --rerun_json_input ${CURRENT_JSON_RERUN}"
              make benchmarks-basic-run \
                  EXTRA_BENCHMARK_FLAGS="--binary_path _build/benchmarks/target/ --output_path ${TARGET_OUTPUT_PATH}/rerun-${i}/ --rerun_json_input ${CURRENT_JSON_RERUN}"
              ./scripts/benchmark-runner.py compare \
                  --baseline_path ${BASELINE_OUTPUT_PATH}/rerun-${i}/ \
                  --target_path ${TARGET_OUTPUT_PATH}/rerun-${i}/ \
                  --rerun_json_output ${NEXT_JSON_RERUN} \
                  --do_not_fail
            done
      - name: "Run Benchmarks - Summary"
        working-directory: 'head'
        run: |
            ./scripts/benchmark-runner.py compare \
                --baseline_path ${BASELINE_OUTPUT_PATH} \
                --target_path ${TARGET_OUTPUT_PATH} \
                --recursive \
                --do_not_fail
      - uses: actions/upload-artifact@v3
        if: always()
        with:
          path: "benchmark-results"
          name: "benchmark-results"
      - uses: actions/upload-artifact@v3
        with:
          path: "/tmp/test_xml_output/"
          name: "test-results"
